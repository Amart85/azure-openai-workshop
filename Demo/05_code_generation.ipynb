{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\r\n",
        "\r\n",
        "Knowledge bases in enterprises are very common in the industry today and can have extensive number of documents in different categories. Retrieving relevant content based on a user query is a challenging task.  Given a query we were able to retrieve information accurately at the document level using methods such as Page Rank developed and made highly accurate especially by Google,  after this point the user has to delve into the document and search for the relevant information.  With recent advances in Foundation Models such as the one developed by Open AI the challenge is alleviated by using “Semantic Search” methods by using encoding information such as “Embeddings” to find the relevant information and then to summarize the content to present to the user in a concise and succinct manner.  \r\n",
        "\r\n",
        "This notebook will introduce the Use Case and will walk you through leveraging Azure Cognitive Search to extract relevant documents and leveraging the power of GPT-3 to address relevant part of the document, and provide a summary based on the prompt (instruction given to the model). It aims to demonstrate how to use Azure OpenAI’s GPT-3 capabilities to adapt to your summarization case, and how to set up and evaluate summarization results. The method is customizable to your summarization use case and can be applied to many different datasets. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "18002aa6-39a1-4e71-8749-324e2f615f31"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Use Case\r\n",
        "\r\n",
        "This use case consists of three sections:\r\n",
        "- Document search\r\n",
        "- Document Zone search\r\n",
        "- Text summarization\r\n",
        "\r\n",
        "Document Search is the process of extracting relevant document based on the query from a corpus of documents.\r\n",
        "Document Zone search is the process of finding the relevant part of the document extracted from document search.\r\n",
        "Text summarization is the process of creating summaries from large volumes of data while maintaining significant informational elements and content value. \r\n",
        "This use case can be useful in helping subject matter experts in finding relevant information from large document corpus.\r\n",
        "Example: In the drug discovery process, scientists in pharmaceutical industry read a corpus of documents to find specific information related to concepts, experiment results etc. This use case enables them to ask questions from the document corpus and the solution will come back with the succinct answer. Consequently, expediting the drug discovery process.\r\n",
        " \r\n",
        "Benefits of the solution:\r\n",
        "1. Shortens reading time\r\n",
        "2. Improves the effectiveness of searching for information\r\n",
        "3. Removes bias from human summarization techniques\r\n",
        "4. Increases bandwidth for humans to focus on more in-depth analysis \r\n",
        "\r\n",
        "\r\n",
        "The need for document summarization be applied to any subject matter (legal, financial, journalist, medical, academic, etc) that requires long document summarization. The subject matter that this notebook is focusing on is journalistic - we will walk through news articles. If the topic gets more domain specific, fine-tuning of the GPT3-model would work better rather than just using the few-shot or zero-shot example methods.  \r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "2f2d0025-3952-481b-9615-cfe5ee198f66"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN daily mail dataset\r\n",
        "For this walkthrough, we will be using the CNN/Daily Mail dataset. This is a common dataset used for text summarization and question answering tasks. Human generated abstractive summary bullets were generated from news stories in CNN and Daily Mail websites.\r\n",
        "\r\n",
        "\r\n",
        "## Data Description\r\n",
        "The relevant schema for our work today consists of:\r\n",
        "\r\n",
        "- id: a string containing the heximal formatted SHA1 hash of the URL where the story was retrieved from\r\n",
        "- article: a string containing the body of the news article\r\n",
        "- highlights: a string containing the highlight of the article as written by the article author\r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "85743c37-40f6-493f-9eaa-e9c4857ba8eb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import python modules"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "d51fdcf4-31f1-4775-950e-2b53c4116e60"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\r\n",
        "from azure.core.credentials import AzureKeyCredential\r\n",
        "from azure.search.documents.indexes import SearchIndexClient \r\n",
        "from azure.search.documents import SearchClient\r\n",
        "import openai\r\n",
        "import re\r\n",
        "import requests\r\n",
        "import sys\r\n",
        "from num2words import num2words\r\n",
        "import os\r\n",
        "import numpy as np\r\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity\r\n",
        "from transformers import GPT2TokenizerFast"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138707943
        }
      },
      "id": "5d042ffa-0868-4fd7-b4db-4886de3ec91c"
    },
    {
      "cell_type": "code",
      "source": [
        "# read the CNN dailymail dataset in pandas dataframe\r\n",
        "df = pd.read_csv('') #path to CNN daily mail dataset\r\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675139533977
        }
      },
      "id": "d3e245f2-3315-4eb1-80d2-80ea22a241c2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Leveraging Cognitivie search to extract relevant article based on the query "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "fc09a779-e3cd-485f-ae3a-297491d993b0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Cognitive Seach Index using CNN Dailymail dataset\r\n",
        "<img src=\"AzureCogSearchIndex.png\" alt=\"Alternative text\" />"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "4ceb9fb6-ea7a-4679-b43e-4abcc6b2a2cf"
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up Azure cognitive service\r\n",
        "service_name = \"\" # Cognitive Search Service Name\r\n",
        "admin_key = \"\" # Cognitive Search Admin Key\r\n",
        "index_name = \"\" # Cognitive Search index name\r\n",
        "\r\n",
        "# Create an SDK client\r\n",
        "endpoint = \"https://{}.search.windows.net\".format(service_name)\r\n",
        "\r\n",
        "search_client = SearchClient(endpoint=endpoint,\r\n",
        "                      index_name=index_name,\r\n",
        "                      api_version=\"2021-04-30-Preview\",\r\n",
        "                      credential=AzureKeyCredential(admin_key))"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138709934
        }
      },
      "id": "7ed0f694-b8c2-408f-90a3-3a763cfbd90e"
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting relevant article based on the query. eg: Clinton Democratic Nomination\r\n",
        "results = search_client.search(search_text=\"Clinton Democratic nomination\", include_total_count=True)\r\n",
        "document = next(results)['article']"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138710195
        }
      },
      "id": "32689db7-4337-42d9-b8f9-4cbd9d98a850"
    },
    {
      "cell_type": "code",
      "source": [
        "document"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675139624461
        }
      },
      "id": "3c9681f2-2448-4e6d-8174-5fb5ff61d5db"
    },
    {
      "cell_type": "code",
      "source": [
        "#length of article extracted from Azure Cognitive search\r\n",
        "len(document) "
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675139635796
        }
      },
      "id": "02375dcd-514e-4203-951e-729b3de07570"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Document Zone Search\r\n",
        "Document Zone: Azure OpenAI Embedding API\r\n",
        "Now that we narrowed on a single document from our knowledge base using Azure Cognitive Search- we can dive deeper into the single document to refine our initial query to a more specific section or \"zone\" of the article.\r\n",
        "\r\n",
        "To do this, we will utilize the Azure Open AI Embeddings API.\r\n",
        "\r\n",
        "## Embeddings Overview\r\n",
        "An embedding is a special format of data representation that can be easily utilized by machine learning models and algorithms. The embedding is an information dense representation of the semantic meaning of a piece of text. Each embedding is a vector of floating-point numbers, such that the distance between two embeddings in the vector space is correlated with semantic similarity between two inputs in the original format. For example, if two texts are similar, then their vector representations should also be similar.\r\n",
        "\r\n",
        "Different Azure OpenAI embedding models are specifically created to be good at a particular task. Similarity embeddings are good at capturing semantic similarity between two or more pieces of text. Text search embeddings help measure long documents are relevant to a short query. Code search embeddings are useful for embedding code snippets and embedding nature language search queries.\r\n",
        "\r\n",
        "Embeddings make it easier to do machine learning on large inputs representing words by capturing the semantic similarities in a vector space. Therefore, we can use embeddings to if two text chunks are semantically related or similar, and inherently provide a score to assess similarity.\r\n",
        "\r\n",
        "## Cosine Similarity\r\n",
        "A previously used approach to match similar documents was based on counting maximum number of common words between documents. This is flawed since as the document size increases, the overlap of common words increases even if the topics differ. Therefore cosine similarity is a better approach.\r\n",
        "\r\n",
        "Mathematically, cosine similarity measures the cosine of the angle between two vectors projected in a multi-dimensional space. This is beneficial because if two documents are far apart by Euclidean distance because of size, they could still have a smaller angle between them and therefore higher cosine similarity.\r\n",
        "\r\n",
        "The Azure OpenAI embeddings rely on cosine similarity to compute similarity between documents and a query."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "30b4b060-1dca-468c-a1f5-ac1b9e5d4878"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Name of models deployed in Azure OpenAI \r\n",
        "<img src=\"Model deployment names.png\" alt=\"Alternative text\" />"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "b3f84fc3-c504-4381-af78-8da5fa0fe10b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Azure OpenAI service and using deployed models"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "1cf78f21-368a-4314-ab59-f5be527e4b08"
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"\" # SET YOUR OWN API KEY HERE\n",
        "RESOURCE_ENDPOINT = \"\" # SET A LINK TO YOUR RESOURCE ENDPOINT\n",
        "openai.api_type = \"azure\"\n",
        "openai.api_key = API_KEY\n",
        "openai.api_base = RESOURCE_ENDPOINT\n",
        "openai.api_version = \"2022-06-01-preview\" #openai api version m\n",
        "\n",
        "TEXT_SEARCH_DOC_EMBEDDING_ENGINE = '' # Model deployment name - mentioned in the above screenshot \n",
        "TEXT_SEARCH_QUERY_EMBEDDING_ENGINE = '' # Model deployment name - mentioned in the above screenshot\n",
        "TEXT_DAVINCI_001 = \"\" # Model deployment name - mentioned in the above screenshot"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1675138710894
        }
      },
      "id": "c1cb38c6"
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining helper functions\n",
        "#Splits text after sentences ending in a period. Combines n sentences per chunk.\n",
        "def splitter(n, s):\n",
        "    pieces = s.split(\". \")\n",
        "    list_out = [\" \".join(pieces[i:i+n]) for i in range(0, len(pieces), n)]\n",
        "    return list_out\n",
        "\n",
        "# Perform light data cleaning (removing redudant whitespace and cleaning up punctuation)\n",
        "def normalize_text(s, sep_token = \" \\n \"):\n",
        "    s = re.sub(r'\\s+',  ' ', s).strip()\n",
        "    s = re.sub(r\". ,\",\"\",s)\n",
        "    # remove all instances of multiple spaces\n",
        "    s = s.replace(\"..\",\".\")\n",
        "    s = s.replace(\". .\",\".\")\n",
        "    s = s.replace(\"\\n\", \"\")\n",
        "    s = s.strip()\n",
        "    \n",
        "    return s"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1675138711079
        }
      },
      "id": "b043fbb4"
    },
    {
      "cell_type": "code",
      "source": [
        "document_chunks = splitter(10, normalize_text(document)) #splitting extracted document into chunks of 10 sentences"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138711316
        }
      },
      "id": "56354758-427f-4af9-94b9-96a25946e9a5"
    },
    {
      "cell_type": "code",
      "source": [
        "document_chunks"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675139752286
        }
      },
      "id": "e3fac902-3761-48f3-b553-557aba1e09f4"
    },
    {
      "cell_type": "code",
      "source": [
        "embed_df = pd.DataFrame(document_chunks, columns = [\"chunks\"]) #datframe with document chunks\r\n"
      ],
      "outputs": [],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138711716
        }
      },
      "id": "39b3c83f-deca-493b-aa41-12b89f24feff"
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an embedding vector for each chunk that will capture the semantic meaning and overall topic of that chunk\r\n",
        "embed_df['curie_search'] = embed_df[\"chunks\"].apply(lambda x : get_embedding(x, engine = TEXT_SEARCH_DOC_EMBEDDING_ENGINE))"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138711984
        }
      },
      "id": "de635ba5-7cf1-4d4f-8598-73619fc9c7ef"
    },
    {
      "cell_type": "code",
      "source": [
        "embed_df #dataframe with document chunks and their embeddings created using Curie embeddings model "
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675139761838
        }
      },
      "id": "0033b178-c09d-4a24-89f7-6c223efc5e71"
    },
    {
      "cell_type": "code",
      "source": [
        "# search through the document for a text segment most similar to the query\n",
        "# display top two most similar chunks based on cosine similarity\n",
        "def search_docs(df, user_query, top_n=3):\n",
        "    embedding = get_embedding(\n",
        "        user_query,\n",
        "        engine=TEXT_SEARCH_QUERY_EMBEDDING_ENGINE\n",
        "    )\n",
        "    df[\"similarities\"] = df.curie_search.apply(lambda x: cosine_similarity(x, embedding))\n",
        "\n",
        "    res = (\n",
        "        df.sort_values(\"similarities\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "        .head(top_n)\n",
        "    )\n",
        "    return res"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138712417
        }
      },
      "id": "7cc7adb8-93dd-4dfd-995a-8df893a98d99"
    },
    {
      "cell_type": "code",
      "source": [
        "document_specific_query = \"trouble so far in clinton campaign\" \n",
        "res = search_docs(embed_df, document_specific_query, top_n=2) #finding top 2 results based on similarity "
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138712650
        }
      },
      "id": "b8511f3a-198f-4e8f-8a5d-cb74456826fa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Summarizer\r\n",
        "\r\n",
        "This section will cover the end-to-end flow of using the GPT-3 models for summarization tasks. \r\n",
        "The model used by the Azure OpenAI service is a generative completion call which uses natural language instructions to identify the task being asked and skill required – aka Prompt Engineering. Using this approach, the first part of the prompt includes natural language instructions and/or examples of the specific task desired. The model then completes the task by predicting the most probable next text. This technique is known as \"in-context\" learning. \r\n",
        "\r\n",
        "There are three main approaches for in-context learning: Zero-shot, Few-shot and Fine tuning. These approaches vary based on the amount of task-specific data that is given to the model: \r\n",
        "\r\n",
        "**Zero-shot**: In this case, no examples are provided to the model and only the task request is provided. \r\n",
        "\r\n",
        "**Few-shot**: In this case, a user includes several examples in the call prompt that demonstrate the expected answer format and content. \r\n",
        "\r\n",
        "**Fine-Tuning**: Fine Tuning lets you tailor models to your personal datasets. This customization step will let you get more out of the service by providing: \r\n",
        "-\tWith lots of data (at least 500 and above) traditional optimization techniques are used with Back Propagation to re-adjust the weights of the model – this enables higher quality results than mere zero-shot or few-shot. \r\n",
        "-\tA customized model improves the few-shot learning approach by training the model weights on your specific prompts and structure. This lets you achieve better results on a wider number of tasks without needing to provide examples in the prompt. The result is less text sent and fewer tokens \r\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "eabac33e-5a98-49f0-8fd6-2750bcf79bb1"
    },
    {
      "cell_type": "code",
      "source": [
        "'''Designing a prompt that will show and tell GPT-3 how to proceed. \r\n",
        "+ Providing an instruction to summarize the text about the general topic (prefix)\r\n",
        "+ Providing quality data for the chunks to summarize and specifically mentioning they are the text provided (context + context primer)\r\n",
        "+ Providing a space for GPT-3 to fill in the summary to follow the format (suffix)\r\n",
        "'''\r\n",
        "\r\n",
        "# result_1 corresponding to the top chunk from Section 2. result_2 corresponding to the second to top chunk from section 2. \r\n",
        "# change index for desired chunk\r\n",
        "result_1 = res.chunks[0]\r\n",
        "result_2 = res.chunks[1]\r\n",
        "prompt_i = 'Summarize the content about the Clinton campaign given the text provided.\\n\\Text:\\n'+\" \".join([normalize_text(result_1)])+ '\\n\\nText:\\n'+ \" \".join([normalize_text(result_2)])+'\\n\\nSummary:\\n'\r\n",
        "\r\n",
        "# Using a low temperature to limit the creativity in the response. \r\n",
        "response = openai.Completion.create(\r\n",
        "        engine= TEXT_DAVINCI_001,\r\n",
        "        prompt = prompt_i,\r\n",
        "        temperature = 0.0,\r\n",
        "        max_tokens = 500,\r\n",
        "        top_p = 1.0,\r\n",
        "        frequency_penalty=0.5,\r\n",
        "        presence_penalty = 0.5,\r\n",
        "        best_of = 1\r\n",
        "    )\r\n",
        "\r\n",
        "print(response.choices[0].text)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "6ec85c16-daec-4eb3-aa33-bed7c20774b6"
    },
    {
      "cell_type": "code",
      "source": [
        "#testing some parameters with a differnt temperature\r\n",
        "response = openai.Completion.create(\r\n",
        "        engine= TEXT_DAVINCI_001,\r\n",
        "        prompt = prompt_i,\r\n",
        "        temperature = 0.2,\r\n",
        "        max_tokens = 500,\r\n",
        "        top_p = 1.0,\r\n",
        "        frequency_penalty=0.5,\r\n",
        "        presence_penalty = 0.5,\r\n",
        "        best_of = 1\r\n",
        "    )\r\n",
        "\r\n",
        "print(response.choices[0].text)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138714150
        }
      },
      "id": "1449c949-1a01-4f20-bebb-e7674ac6de43"
    },
    {
      "cell_type": "code",
      "source": [
        "#testing some parameters \r\n",
        "response = openai.Completion.create(\r\n",
        "        engine= TEXT_DAVINCI_001,\r\n",
        "        prompt = prompt_i,\r\n",
        "        temperature = 0.7,\r\n",
        "        max_tokens = 500,\r\n",
        "        top_p = 1.0,\r\n",
        "        frequency_penalty=0.5,\r\n",
        "        presence_penalty = 0.5,\r\n",
        "        best_of = 1\r\n",
        "    )\r\n",
        "\r\n",
        "print(response.choices[0].text)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1675138715196
        }
      },
      "id": "47147cfe-ad88-4c97-b6eb-0a9f82f0bd91"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}