{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Azure OpenAI for Big Data\r\n",
        "\r\n",
        "The Azure OpenAI service can be used to solve a large number of natural language tasks through prompting the completion API. To make it easier to scale your prompting workflows from a few examples to large datasets of examples we have integrated the Azure OpenAI service with the distributed machine learning library [SynapseML](https://www.microsoft.com/en-us/research/blog/synapseml-a-simple-multilingual-and-massively-parallel-machine-learning-library/). This integration makes it easy to use the [Apache Spark](https://spark.apache.org/) distributed computing framework to process millions of prompts with the OpenAI service. This tutorial shows how to apply large language models at a distributed scale using Azure Open AI and Azure Synapse Analytics. \r\n",
        "\r\n",
        "## Step 1: Prerequisites\r\n",
        "\r\n",
        "The key prerequisites for this quickstart include a working Azure OpenAI resource, and an Apache Spark cluster with SynapseML installed. We suggest creating a Synapse workspace, but an Azure Databricks, HDInsight, or Spark on Kubernetes, or even a python environment with the `pyspark` package will work. \r\n",
        "\r\n",
        "1. An Azure OpenAI resource â€“ request access [here](https://customervoice.microsoft.com/Pages/ResponsePage.aspx?id=v4j5cvGGr0GRqy180BHbR7en2Ais5pxKtso_Pz4b1_xUOFA5Qk1UWDRBMjg0WFhPMkIzTzhKQ1dWNyQlQCN0PWcu) before [creating a resource](https://docs.microsoft.com/en-us/azure/cognitive-services/openai/how-to/create-resource?pivots=web-portal#create-a-resource)\r\n",
        "1. [Create a Synapse workspace](https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-create-workspace)\r\n",
        "1. [Create a serverless Apache Spark pool](https://docs.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-spark#create-a-serverless-apache-spark-pool)\r\n",
        "\r\n",
        "\r\n",
        "## Step 2: Import this guide as a notebook\r\n",
        "\r\n",
        "The next step is to add this code into your Spark cluster. You can either create a notebook in your Spark platform and copy the code into this notebook to run the demo. Or download the notebook and import it into Synapse Analytics\r\n",
        "\r\n",
        "1.\t[Download this demo as a notebook](https://github.com/microsoft/SynapseML/blob/master/notebooks/features/cognitive_services/CognitiveServices%20-%20OpenAI.ipynb) (click Raw, then save the file)\r\n",
        "1.\tImport the notebook [into the Synapse Workspace](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks#create-a-notebook) or if using Databricks [into the Databricks Workspace](https://docs.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage#create-a-notebook)\r\n",
        "1. Install SynapseML on your cluster. Please see the installation instructions for Synapse at the bottom of [the SynapseML website](https://microsoft.github.io/SynapseML/). Note that this requires pasting an additional cell at the top of the notebook you just imported\r\n",
        "3.\tConnect your notebook to a cluster and follow along, editing and rnnung the cells below.\r\n",
        "\r\n",
        "## Step 3: Fill in your service information\r\n",
        "\r\n",
        "Next, please edit the cell in the notebook to point to your service. In particular set the `service_name`, `deployment_name`, `location`, and `key` variables to match those for your OpenAI service:"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\r\n",
        "pip install datasets"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\r\n",
        "from pyspark.sql.functions import  col, udf,mean,  concat, lit\r\n",
        "from pyspark.sql import functions as F\r\n",
        "from nltk.translate.bleu_score import sentence_bleu\r\n",
        "from notebookutils import mssparkutils\r\n",
        "from notebookutils import mssparkutils\r\n",
        "\r\n",
        "\r\n",
        "dataset = load_dataset('opus_books','en-fr')\r\n",
        "df = dataset['train'].to_pandas().head(5000)\r\n",
        "# Convert to Spark dataframe\r\n",
        "df = spark.createDataFrame(df)\r\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.select('id', F.col('translation').getItem('fr').alias('fr'), F.col('translation')['en'].alias('en'))\r\n",
        "df = df.select(concat(lit(\"Translate this sentence to English: \"), col('fr')).alias('fr'), 'en')\r\n",
        "display(df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\r\n",
        "# Fill in the following lines with your service information\r\n",
        "service_name = \"CogSearchDemo\"\r\n",
        "deployment_name = \"text-ada-001\"\r\n",
        "key=mssparkutils.credentials.getSecret('mldemo4764731797' , 'OPENAI-KEY') # get OpenAI key from KeyVault"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the OpenAICompletion Apache Spark Client\r\n",
        "\r\n",
        "To apply the OpenAI Completion service to your dataframe you just created, create an OpenAICompletion object which serves as a distributed client. Parameters of the service can be set either with a single value, or by a column of the dataframe with the appropriate setters on the `OpenAICompletion` object. Here we are setting `maxTokens` to 200. A token is around 4 characters, and this limit applies to the sum of the prompt and the result. We are also setting the `promptCol` parameter with the name of the prompt column in the dataframe."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from synapse.ml.cognitive import OpenAICompletion\r\n",
        "\r\n",
        "completion = (\r\n",
        "    OpenAICompletion()\r\n",
        "    .setSubscriptionKey(key)\r\n",
        "    .setDeploymentName(deployment_name)\r\n",
        "    .setUrl(\"https://{}.openai.azure.com/\".format(service_name))\r\n",
        "    .setMaxTokens(200)\r\n",
        "    .setPromptCol(\"fr\")\r\n",
        "    .setErrorCol(\"error\")\r\n",
        "    .setOutputCol(\"translated_en\")\r\n",
        ")\r\n",
        "\r\n",
        "# To process 5000 sentences, it takes 25 minutes using small cpu cluster\r\n",
        "# completed_df = completion.transform(df).cache()\r\n",
        "completed_df = completion.transform(df).select(\r\n",
        "        col(\"fr\"),\r\n",
        "        col(\"error\"),\r\n",
        "        col(\"translated_en.choices.text\").getItem(0).alias(\"text\"),\r\n",
        "        col('en')\r\n",
        ")\r\n",
        "completed_df = completed_df.select(col(\"fr\"), col(\"text\"), col(\"en\"),bleu(\"text\",\"en\").alias(\"bleu\")).cache()\r\n",
        "display(completed_df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TO DO: \r\n",
        "# Write code to measure the performance of the translation"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}